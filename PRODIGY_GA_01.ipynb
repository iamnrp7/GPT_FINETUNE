{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **STEP 1: INSTALL NECESSARY PACKAGES**  \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ev0vEz6hWXco"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fEEvXzFWUOQ",
        "outputId": "5a3d4594-ea0b-4855-e80a-210862e0274b",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q gradio\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **STEP 2: IMPORT REQUIRED LIBRARIES**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "n9bGh20Mrinv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import tensorflow as tf\n",
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n"
      ],
      "metadata": {
        "id": "vfRbcsJ8Y-wB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **STEP 3: LOAD THE MODEL AND TOKENIZER**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "wC3TPwdaZmpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkle2nPeZq0H",
        "outputId": "29e84c68-ab6e-4f34-f06e-993c3fe70110",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **STEP 4: DEFINE THE TEXT GENERATION FUNCTION**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Zezod03Mrwh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_text(input_text):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='tf')\n",
        "    beam_output = model.generate(input_ids, max_length=100, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
        "    output_text = tokenizer.decode(beam_output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "    return \".\".join(output_text.split(\".\")[:-1]) + \".\"\n"
      ],
      "metadata": {
        "id": "NilJ8NSQjXJ0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **STEP 5: CREATE THE GRADIO INTERFACE AND LAUNCH IT**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TT9qt09ulSb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_text = gr.Textbox()\n",
        "gr.Interface (generate_text, \"textbox\", output_text, title=\"GPT-2\",\n",
        "              description=\"OpenAI's GPT-2 is an unsupervised language model that \\\n",
        "              can generate coherent text. Go ahead and input a sentence and see what it completes \\\n",
        "              it with! Takes around 20s toÂ run.\").launch()"
      ],
      "metadata": {
        "id": "o0Qpi8JZlxeZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "collapsed": true,
        "outputId": "26e66d98-7458-4158-f57d-70d11dc55495"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://4e8f419d89b40d4be7.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4e8f419d89b40d4be7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}
